# Reading Data in Python

Sometimes the size of the data can be so large that it is impossible to keep all of it in memory. In this case, we can read the data as chunks. If this data is in csv file format, we can use the code block below.

```
def read_data(csv_file, c_size, colname):

    # Iterate over the file chunk by chunk
    for chunk in pd.read_csv(csv_file, chunksize=c_size):

        # Iterate over the any column you want in DataFrame
        for entry in chunk[colname]:
```

### Read_csv

If we declare date column as datetime in pandas, in visualizing matplotlib recognizes this type and makes appropriate segments automatically. When reading we can set the datatype like this: `pd.read_csv('climate_change.csv', parse_dates=["date"], index_col="date")`. Important tip is that parse_dates argument takes sequences as input. 

`pd.read_csv('file.csv', index_col=0)`: Makes first column index column. 

### Pickled files

- File type native to Python
- There many datatypes such as dictionaries and lists, for which it is not obvious as dataframes to how to save them. 
- JSON's are appropriate way to save dictionaries. But to import them to Python we need to serialize aka pickling them. Serializing means to convert objects to sequence of bytes or bystreams. 

### Generators vs. List Comprehensions

Generators are useful when we want to go over very large list. Because they use lazy evaluation this means that evaluation of the expression is delayed until its value is needed. 
```
result = [num for num in range(10**100000000)]
```
The list below would first create the list then if we want to go over it we need additional for loops. but the number of elements in the list is so large that our computer or server cannot even create such a large list.

The generator version is below. To iterate over generators we can use for loops as well. 

```
result = (num for num in range(3))

print(next(result)) #0
print(next(result)) #1
..
```
### Generator functions

These are the functions that produce generator objects. Instead of using return to returning values these functions uses keyword yield.  

The difference between yield and return is that yield returns a value and pauses the execution while maintaining the internal states, whereas the return statement returns a value and terminates the execution of the function.

# Reading Data from Web

Some useful packages:

- `urllib`: Provides interface for fetching data across the web. 

For example if we have .csv formatted data in web we can import it to python like this:

```
from urllib.request import urlretrieve
url = "url_link.csv"
# Saving file locally (you can also directly save to dataframe)
urlretrieve(url, "save.csv")
```

Getting the HTML data from a webpage

```
# With request package (suggested way)
import requests
r = requests.get("url_link")
text = r.text

# Second way
from urllib.request import urlopen, Request
response = urlopen(Request("url_link"))
html = response.read()
response.close()
```

BeautifulSoup is useful to parse and extract structured data from HTML. Tag soup is structurally or syntactically incorrect HTML code. BeautifulSoup makes tag soup more user friendly, readable format. 

```
from bs4 import BeautifulSoup
import requests

r = requests.get("url_link")
text = r.text

soup = BeautifulSoup(text)

# Prints html with indents
print(soup.prettify())

# Gets title
print(soup.title)

# Gets text
print(soup.get_text())

# Get all hyperlinks
for link in soup.find_all('a'):
    print(link.get('href'))
```









