## General DataFrame Methods

`df.info()`: Displays the names of columns, the data types they contain and whether they have any missing values.

`df.describe()`: Some summary statistics for numerical columns. 

`df.drop_duplicates(subset='col1')`: Drops non-unique rows from the column col1. subset also could be list of columns like `subset=['col1', 'col2']`

`df.duplicated()`: Returns True, False series whether value is duplicated or not. (But with this search all of the columns must contain duplicated results.) `subset`: List of column names to check for duplication. `keep`: Whether to keep first
(`first`), last (`last`) and all (`False`) duplicate values. 
      
Sometimes we may have data that is nearly identical except 1 or 2 values and we can use statistical measurements to achieve one result. For example if we have record about same human but with different height and weight, we can use statistics like this:

```
columns= ['first_name', 'last_name', 'address']
height_weight = height_weight.groupby(columns).agg({'height':'max', 'weight':'mean'}).reset_index()

# Check for duplicates
duplicates = height_weight.duplicated(subset=columns , keep=False)
height_weight[duplicates].sort_values(by='first_name')
```
 
 
`df.set_index('col1')`: Changes the index from default 0, 1, 2, .. to col1 values. To undo the change use `df.reset_index()`, if you want to reset and get rid of col1 then you can `df.reset_index(drop=True)`

`df.set_index(['col1', 'col2'])`: Multi-level (hierarchical) indexes. They are useful when we have hierarchical columns like country, city, etc.

`df.loc[('col1_value1', 'col2_value1'):('col1_value2', 'col2_value2')]`: Subset inner levels with a list of tuples. 

`df.loc[('col1_value1', 'col2_value1'):('col1_value2', 'col2_value2'), col3:col4]`: Subset inner level rows and columns col3 to col4. 

`dataframe["column"].dt.component`: Accessing the components of a date (year, month and day). For month `dataframe["column"].dt.month`.

`display()`: It is not directly related method to dataframes but it is very useful when we want to print our dataframes, it allows us to display more than one cell in the output when making multiple queries around dataframe. 

`df.nlargest(1, "col_name")` or `df.nsmallest(1, "col_name")`: Getting max or min value for column in dataframe. 

### Missing values

`df.isna()`: Returns dataframe consisting of True/False based on missing values. (for each value)

`df.isna().any()`: For each column returns whether column has missing values or not. 

`df.isna().sum()`: Returns number of missing values in each column. To show it in bar plot use `df.isna().sum().plot(kind='bar')`

`df.dropna()`: Removes rows with missing values.  

`df.fillna(another_value)`: Replaces missing values with another value.

## General DataFrame Attributes

`df.shape`: Returns a tuple that contains number of rows and columns. 

`df.values`: Data values in a 2-dimensional numpy array.

`df.columns`: Column names. 

`df.index`: Row numbers or row names. 

`df.dtypes`: Checking datatype of each column. 

## Creating DataFrames

There are 2 methods to building dataframes. 

1. From a list of dictionaries
   
   Constructed row by row
   
    ```
    list_of_dicts = [{"col1":"val1", "col2":"val2"},
                     {"col1":"val3", "col2":"val4"}]
    ```

2. From a dictionary of lists

   Constructed column by column

    ```
    dict_of_lists = {"col1": ["val1", "val3"],
                     "col2": ["val2", "val4"]}
    ```

Then call `pd.DataFrame(list_of_dicts)` or `pd.DataFrame(dict_of_lists)`

## Sorting and Subsetting

`df.sort_values('col_name')`: Change the order of the rows by sorting them according to column values. 

`df.sort_values(['col1', 'col2'])`: Sorting by muliple variables. First sorting by col1 then by col2. 

`df.sort_values(['col1', 'col2'], ascending=[True, True])`: Changing direction of sorting. 

`df.sort_index()`: Sorts by index.

`df[['col1', 'col2']]`: Getting only col1 and col2 values. 

`df[df['col_name'] > 10]`: Subsetting rows based on one condition.

`df[(df['col1'] > 10) & (df['col2'] == 'example')]`: Subsetting rows based on multiple conditions.

`df[df['col_name'].isin(['Blue', 'Red'])]`: Subsetting using isin(), getting rows which contain Blue and Red values in column col_name.

`df.loc[['Blue', 'Red']]`: Same output as above but shorter and common way.

### Slicing and subsetting with loc and iloc

- You can only slice an index if the index is sorted. 

- In loc both values are included, in iloc first index value is included but the last is not included like in range.

`df.loc['starting_val':'ending_val']`: Gets rows from starting_val to ending_val including both values. (if these values are in index column)

`df.loc[:, 'starting_col':'ending_col']`: Gets all rows but columns that are starting with starting_col, ending with ending_col. 

`df.iloc[1:2, 3:6]`: Slicing with row and column numbers. Last indexes (2 and 6) are not included.

### Selecting data with .query()

`df.query('col1 >= 10')`: Querying on a single condition. Like SQL but only part after WHERE. 

`df.query('col1 >= 10 and col2 < 5')`: Querying on a multiple conditions. 

## Agg method

With agg method we can see cumulative statistics for one or more columns. In code block below iqr method is defined and passed to agg method. 

```
def iqr(column):
    return column.quantile(0.75) - column.quantile(0.25)

print(df[["col1", "col2", "col3"]].agg([iqr, np.median]))
```

We can use agg method with group by to perform operations on grouped values. In agg method we can define different summary statistics like max, min.

```
df.groupby('col1').agg({'col2':'count'})
```

## Summary Statistics

`df['col1'].value_counts(sort=True)`: Counts number of same values then sorts them in descending order. (with `normalize=True` proportions can be achieved) 

### Summaries by group

```
df[df['color'] == 'Black']['price'].mean()
df[df['color'] == 'Red']['price'].mean()
...
```
We know that we find the average of the prices according to the colors in the code snippet given above, but when we have too much color data, using such a structure causes code duplication.

`df.groupby('color')['price'].mean()`: With the group by method, we can reduce this to a single line. 

`df.groupby(['color', 'type'])['price'].mean()`: By giving list instead of just one column in group by we can also group by multiple columns. 

`df.groupby(['color', 'type'])['price', 'revenue'].mean()`: You can also aggregate by multiple columns. 

`df.groupby('color')['price'].agg([min, max, sum])`: We can use the agg method to get multiple statistics. 

### Pivot tables

Second method for groupby is to use pivot tables. 

`df.pivot_table(values='price', index='color')` and `df.groupby('color')['price'].mean()` is gives us the same result. 

In pivot tables we did not specify the statistics because default one is `mean`. But if we want to display another statistics we can do it like `df.pivot_table(values='price', index='color', aggfunc=np.median)` this. If we want multiple summary statistics we can pass a list of functions to the aggfunc argument. 

`df.pivot_table(values='price', index='color', columns='type'`: In group by we could also group by multiple columns. In pivot tables we can also achieve same result like this. The difference that pivot table would also include NaN values. Instead of NaN's with `fill_value=0` we can replace this values with 0's. With `margins=True` last column would include mean values for each row (not including NaN values). 

`df.mean(axis='index')` or `df.mean()`: Calculates mean for each column. 

`df.mean(axis='columns')`: Calculates mean for each row. 

### Melting tables

In wide data format, every row relates to one subject. Each column has different information about an attribute of that subject. 

In long data format, information about subject is found over many rows. 

<img src="https://imgur.com/KVcSvRh.png" width="50%"> 

Wide format is easier to read and understand but long formatted data is often more accessible for computers to work with. 

Melt method will allow us to unpivot our dataset. Changing format from wide to long. 

`df.melt(id_vars=['col1', 'col2'])`: Columns in id_vars are the columns that we do not want to change. `value_vars = ['col3', 'col4']` Only unpivotting col3 and col4 values. 

For example if we have columns from col1 to col 6 and our method is `df.melt(id_vars=['col1', 'col2'], value_vars = ['col3', 'col4'])` then col5 and col6 would not be in our final representation. 

With `var_name='col_name'` and `value_name='col2_name'` we can change column names which are by default variable and value. 


## Useful methods 

### 1. Apply

Useful when we need to apply method to every row in dataframe. In code below we are cleaning some columns which are containing special characters. 

```
# List of characters to remove
chars_to_remove = ["+", ",", "$"]
# List of column names to clean
cols_to_clean = ["Installs", "Price"]

# Loop for each column in cols_to_clean
for col in cols_to_clean:
    # Loop for each char in chars_to_remove
    for char in chars_to_remove:
        # Replace the character with an empty string
        apps[col] = apps[col].apply(lambda x: x.replace(char, ''))      
```

### 2. Unique

Useful when we need to reach unique data (or number of unique elements) in columns. 

`df['Category'].unique()`: Returns unique values in Category column. `len(apps['Category'].unique())`

### 3. Value counts

`df['Category'].value_counts()`: Finds how many times each value in this column is presented. 

## Cleaning Data

`df['col1'] = df['col1'].str.strip('$').astype('int')`: Remove signs from integer or float values to make calculations

`df.(df[df['col'] > 10].index, inplace = True)`: Dropping values based on condition. 


