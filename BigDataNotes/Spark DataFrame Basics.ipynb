{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9925041",
   "metadata": {},
   "source": [
    "# Spark DataFrame Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "3a39e40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "70c5a8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('Basics').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f935a1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.json('people.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a265eece",
   "metadata": {},
   "source": [
    "## 1. Basic Methods or Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "2e36f5ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "968372cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "0bb1b830",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['age', 'name']"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "1c2f1196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-------+\n",
      "|summary|               age|   name|\n",
      "+-------+------------------+-------+\n",
      "|  count|                 2|      3|\n",
      "|   mean|              24.5|   null|\n",
      "| stddev|7.7781745930520225|   null|\n",
      "|    min|                19|   Andy|\n",
      "|    max|                30|Michael|\n",
      "+-------+------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4148e360",
   "metadata": {},
   "source": [
    "## 2. Changing Type of Columns (Optional)\n",
    "\n",
    "Use this part if Spark can not figure out correct types by itself. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "87b3f307",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import (StructField, StringType, \n",
    "                               IntegerType, StructType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "ecaa895a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters of StructField: \n",
    "# 1. The column whose type we want to change,\n",
    "# 2. The new type of the column,\n",
    "# 3. Can values in this column be nullable\n",
    "data_schema = [StructField('age', IntegerType(), True),\n",
    "              StructField('name', StringType(), True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d5898b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_struc = StructType(fields=data_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "86ffcd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.json('people.json', schema=final_struc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "4b0c876d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99abd98",
   "metadata": {},
   "source": [
    "## 3. Select Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "a515427d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.column.Column"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Can not view the contents of the column\n",
    "type(df['age'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "1e09c8ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "| age|\n",
      "+----+\n",
      "|null|\n",
      "|  30|\n",
      "|  19|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Type of df.select('age') is dataFrame hence we can see the contents\n",
    "df.select('age').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "f114d7a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(['age', 'name']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "2d88c0b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(age=None, name='Michael')"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "3ef4f82d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.types.Row"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " type(df.head(2)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a13769f",
   "metadata": {},
   "source": [
    "There are many specialized types in Spark such as **pyspark.sql.column.Column** or **pyspark.sql.types.Row.** In typical Python projects types are a lot simplier like lists. \n",
    "\n",
    "Main reason behind this that Spark reads data from many distributed data sources and then do computational part on distributed computers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce8ed46",
   "metadata": {},
   "source": [
    "## 4. Adding New Data\n",
    "\n",
    "Both examples are not changing the main dataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "7b06cc5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+----------+\n",
      "| age|   name|double_age|\n",
      "+----+-------+----------+\n",
      "|null|Michael|      null|\n",
      "|  30|   Andy|        60|\n",
      "|  19| Justin|        38|\n",
      "+----+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('double_age', df['age']*2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "a7966b9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "6e689a22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+\n",
      "|new_age|   name|\n",
      "+-------+-------+\n",
      "|   null|Michael|\n",
      "|     30|   Andy|\n",
      "|     19| Justin|\n",
      "+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumnRenamed('age', 'new_age').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "e6d6b876",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d97b295",
   "metadata": {},
   "source": [
    "## 5. Filtering Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "c891b83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = spark.read.csv('appl_stock.csv', inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "ef699e68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: timestamp (nullable = true)\n",
      " |-- Open: double (nullable = true)\n",
      " |-- High: double (nullable = true)\n",
      " |-- Low: double (nullable = true)\n",
      " |-- Close: double (nullable = true)\n",
      " |-- Volume: integer (nullable = true)\n",
      " |-- Adj Close: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "0df84724",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(Date=datetime.datetime(2010, 1, 4, 0, 0), Open=213.429998, High=214.499996, Low=212.38000099999996, Close=214.009998, Volume=123432400, Adj Close=27.727039)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.head(3)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "9b355857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|      Open|\n",
      "+----------+\n",
      "|213.429998|\n",
      "|214.599998|\n",
      "|214.379993|\n",
      "+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.filter(\"Close < 500\").select(\"Open\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "169d01cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|      Open|\n",
      "+----------+\n",
      "|213.429998|\n",
      "|214.599998|\n",
      "|214.379993|\n",
      "+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This is more pythonic way to filter out the data \n",
    "df2.filter(df2['CLose'] < 500).select('Open').show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afcb0afd",
   "metadata": {},
   "source": [
    "### 5.1 Filtering on multiple conditions\n",
    "\n",
    "**df2.filter(df2['Close'] < 200 & df2['Open'] > 200).show()**: Results in Py4JError. \n",
    "Be careful about parenthesis. Also typical and, or does not work in filtering, you need to use & and | operators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "c18eff7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+----------+------------------+----------+---------+------------------+\n",
      "|               Date|              Open|      High|               Low|     Close|   Volume|         Adj Close|\n",
      "+-------------------+------------------+----------+------------------+----------+---------+------------------+\n",
      "|2010-02-01 00:00:00|192.36999699999998|     196.0|191.29999899999999|194.729998|187469100|         25.229131|\n",
      "|2010-02-02 00:00:00|        195.909998|196.319994|193.37999299999998|195.859997|174585600|25.375532999999997|\n",
      "+-------------------+------------------+----------+------------------+----------+---------+------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.filter((df2['Close'] < 200) & ~(df2['Open'] > 200)).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048b4307",
   "metadata": {},
   "source": [
    "### 5.2 Access values of data\n",
    "\n",
    "The example above only showing the required data but if we want to make some calculations with it we need to access it directly. To access the data use the **collect** method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "ae4266b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = df2.filter(df2['Low'] == 197.16).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "b57a1214",
   "metadata": {},
   "outputs": [],
   "source": [
    "row = result[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "26c078d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "220441900"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Type of the row above is pyspark.sql.types.Row\n",
    "# To access the data change the type to dictionary\n",
    "# Or you can also access this data by typing row[5]\n",
    "row.asDict()['Volume']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1d4f5c",
   "metadata": {},
   "source": [
    "## 6. GroupBy and Aggregate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "b31f2b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = spark.read.csv('sales_info.csv', inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "be9ec5b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-----+\n",
      "|Company| Person|Sales|\n",
      "+-------+-------+-----+\n",
      "|   GOOG|    Sam|200.0|\n",
      "|   GOOG|Charlie|120.0|\n",
      "|   GOOG|  Frank|340.0|\n",
      "|   MSFT|   Tina|600.0|\n",
      "|   MSFT|    Amy|124.0|\n",
      "|   MSFT|Vanessa|243.0|\n",
      "|     FB|   Carl|870.0|\n",
      "|     FB|  Sarah|350.0|\n",
      "|   APPL|   John|250.0|\n",
      "|   APPL|  Linda|130.0|\n",
      "|   APPL|   Mike|750.0|\n",
      "|   APPL|  Chris|350.0|\n",
      "+-------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "f0984f6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Company: string (nullable = true)\n",
      " |-- Person: string (nullable = true)\n",
      " |-- Sales: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "58567ae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+\n",
      "|Company|max(Sales)|\n",
      "+-------+----------+\n",
      "|   APPL|     750.0|\n",
      "|   GOOG|     340.0|\n",
      "|     FB|     870.0|\n",
      "|   MSFT|     600.0|\n",
      "+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.groupBy(\"Company\").max().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98f3138",
   "metadata": {},
   "source": [
    "You can also write the code above like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "856b30ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+\n",
      "|Company|max(Sales)|\n",
      "+-------+----------+\n",
      "|   APPL|     750.0|\n",
      "|   GOOG|     340.0|\n",
      "|     FB|     870.0|\n",
      "|   MSFT|     600.0|\n",
      "+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "grouped_data = df3.groupBy(\"Company\")\n",
    "grouped_data.agg({'Sales':'max'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "03f8f0a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|max(Sales)|\n",
      "+----------+\n",
      "|     870.0|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.agg({'Sales':'max'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "53cd282c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import countDistinct, avg, stddev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "494d7c32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+\n",
      "|count(DISTINCT Sales)|\n",
      "+---------------------+\n",
      "|                   11|\n",
      "+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.select(countDistinct('Sales')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "62e0fd8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|    Average Sales|\n",
      "+-----------------+\n",
      "|360.5833333333333|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.select(avg('Sales').alias('Average Sales')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "027a8720",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_std = df3.select(stddev('Sales').alias('std'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2e12ca",
   "metadata": {},
   "source": [
    "As you can see the values are really long for average and std. To format these values use the method given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "6185f483",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import format_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "e57eaa67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|   std|\n",
      "+------+\n",
      "|250.09|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_std.select(format_number('std',2).alias('std')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed40fdc3",
   "metadata": {},
   "source": [
    "ORDER BY ASCENDING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "adcae19c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-----+\n",
      "|Company| Person|Sales|\n",
      "+-------+-------+-----+\n",
      "|   GOOG|Charlie|120.0|\n",
      "|   MSFT|    Amy|124.0|\n",
      "|   APPL|  Linda|130.0|\n",
      "|   GOOG|    Sam|200.0|\n",
      "|   MSFT|Vanessa|243.0|\n",
      "|   APPL|   John|250.0|\n",
      "|   GOOG|  Frank|340.0|\n",
      "|     FB|  Sarah|350.0|\n",
      "|   APPL|  Chris|350.0|\n",
      "|   MSFT|   Tina|600.0|\n",
      "|   APPL|   Mike|750.0|\n",
      "|     FB|   Carl|870.0|\n",
      "+-------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.orderBy('Sales').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d59878c",
   "metadata": {},
   "source": [
    "ORDER BY DESCENDING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "318e1407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-----+\n",
      "|Company| Person|Sales|\n",
      "+-------+-------+-----+\n",
      "|     FB|   Carl|870.0|\n",
      "|   APPL|   Mike|750.0|\n",
      "|   MSFT|   Tina|600.0|\n",
      "|     FB|  Sarah|350.0|\n",
      "|   APPL|  Chris|350.0|\n",
      "|   GOOG|  Frank|340.0|\n",
      "|   APPL|   John|250.0|\n",
      "|   MSFT|Vanessa|243.0|\n",
      "|   GOOG|    Sam|200.0|\n",
      "|   APPL|  Linda|130.0|\n",
      "|   MSFT|    Amy|124.0|\n",
      "|   GOOG|Charlie|120.0|\n",
      "+-------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.orderBy(df3['Sales'].desc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08cdcd73",
   "metadata": {},
   "source": [
    "## 7. Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "e4c1ab64",
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = spark.read.csv('ContainsNull.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "e12a2668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp1| John| null|\n",
      "|emp2| null| null|\n",
      "|emp3| null|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "e944d155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df4.na.drop().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677a0d25",
   "metadata": {},
   "source": [
    "### 7.1 Drop missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "a2e33dbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp1| John| null|\n",
      "|emp3| null|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Row needs to have at least 2 non-null value in order to keep it \n",
    "df4.na.drop(thresh=2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "6c5ee923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp1| John| null|\n",
      "|emp2| null| null|\n",
      "|emp3| null|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Any or all\n",
    "df4.na.drop(how='all').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "cb73edc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp3| null|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df4.na.drop(subset=['Sales']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38eabeb",
   "metadata": {},
   "source": [
    "### 7.2 Fill missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "03df0866",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Id: string (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Sales: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df4.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "3872b9f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+-----+\n",
      "|  Id|      Name|Sales|\n",
      "+----+----------+-----+\n",
      "|emp1|      John| null|\n",
      "|emp2|FILL VALUE| null|\n",
      "|emp3|FILL VALUE|345.0|\n",
      "|emp4|     Cindy|456.0|\n",
      "+----+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fills only values whose type is string\n",
    "df4.na.fill('FILL VALUE').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "30111021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp1| John|  0.0|\n",
      "|emp2| null|  0.0|\n",
      "|emp3| null|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fills only values whose type is numeric\n",
    "df4.na.fill(0).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836aa833",
   "metadata": {},
   "source": [
    "<font color='RED'>IT IS A GOOD PRACTICE TO SPECIFY COLUMNS WHEN FILLING THEM AND NOT TRUST SPARK TO INFER THE TYPE.</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "2d60b84e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+-----+\n",
      "|  Id|   Name|Sales|\n",
      "+----+-------+-----+\n",
      "|emp1|   John| null|\n",
      "|emp2|No Name| null|\n",
      "|emp3|No Name|345.0|\n",
      "|emp4|  Cindy|456.0|\n",
      "+----+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df4.na.fill('No Name', subset=['Name']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324a1faa",
   "metadata": {},
   "source": [
    "<font color='RED'>ANOTHER COMMON PRACTICE IS TO FILL THE VALUES WITH MEAN VALUE.</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "c3f6304b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "2efd0daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_val = df4.select(mean(df4['Sales'])).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "b63c67a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp1| John|400.5|\n",
      "|emp2| null|400.5|\n",
      "|emp3| null|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df4.na.fill(mean_val[0][0], ['Sales']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d11032",
   "metadata": {},
   "source": [
    "## 8. Spark SQL \n",
    "\n",
    "**createOrReplaceTempView** creates (or replaces if that view name already exists) a lazily evaluated \"view\" that you can then use like a hive table in Spark SQL. It does not persist to memory unless you cache the dataset that underpins the view. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "0410eee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView('people')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "0d806ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = spark.sql(\"SELECT * FROM people WHERE age=30\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "5c07cbef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "|age|name|\n",
      "+---+----+\n",
      "| 30|Andy|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c39823",
   "metadata": {},
   "source": [
    "## 9. Dates and Timestamps\n",
    "\n",
    "For this section old dataset which was transferred to df2 is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "3b6fbb3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Date=datetime.datetime(2010, 1, 4, 0, 0), Open=213.429998, High=214.499996, Low=212.38000099999996, Close=214.009998, Volume=123432400, Adj Close=27.727039)]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "003361a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import (dayofmonth, year, month, format_number, date_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "72970348",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|dayofmonth(Date)|\n",
      "+----------------+\n",
      "|               4|\n",
      "|               5|\n",
      "|               6|\n",
      "|               7|\n",
      "|               8|\n",
      "+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.select(dayofmonth(df2['Date'])).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "24ae8a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = df2.withColumn(\"Year\", year(df2['Date']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "8949abb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = new_df.groupBy('Year').mean().select(['Year', 'avg(Close)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "83b7d8ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+\n",
      "|Year|Avg Close|\n",
      "+----+---------+\n",
      "|2015|   120.04|\n",
      "|2013|   472.63|\n",
      "|2014|   295.40|\n",
      "|2012|   576.05|\n",
      "|2016|   104.60|\n",
      "|2010|   259.84|\n",
      "|2011|   364.00|\n",
      "+----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result.select(['Year', format_number('avg(Close)', 2).alias(\"Avg Close\")]).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
